{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_-5btKM8Uvl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from IPython import display\n",
        "\n",
        "try:\n",
        "  import torch\n",
        "\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "  %pip install -qq torch\n",
        "  import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "import hashlib\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(seed=1)\n",
        "torch.manual_seed(seed=1)\n",
        "\n",
        "!mkdir figures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**"
      ],
      "metadata": {
        "id": "1OSIEovOevb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "  \"\"\"An interator to load sequence data.\"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = self.seq_data_iter_sequential\n",
        "\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "  \"\"\"Vocabulary for text.\"\"\"\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "    counter = count_corpus(tokens)\n",
        "    self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    self.unk, uniq_tokens = 0, [\"<unk>\"] + reserved_tokens\n",
        "    uniq_tokens += [token for token, freq in self.token_freqs if freq >= min_freq and token not in uniq_tokens]\n",
        "    self.idx_to_token, self.tooken_to_idx = [], dict()\n",
        "\n",
        "    for token in uniq_tokens:\n",
        "      self.idx_to_token.append(token)\n",
        "      self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens,(list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    retuns [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    return [self.idx_to_token[index] for index in indices]\n"
      ],
      "metadata": {
        "id": "gOzSMbssdLim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):\n",
        "  if token == 'word':\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == 'char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print(\"ERROR: unknown token type: \" + token)\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  \"\"\"Count token frequencies.\"\"\"\n",
        "\n",
        "  if len(tokens) == 0 or isinstance(tokens[0],list):\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)\n",
        "\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generate mini-batches using random sampling.\"\"\"\n",
        "  # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "  # sequence\n",
        "  corpus = corpus[random.randint(0, num_step - 1) :]\n",
        "  # Subtract 1 since we need to account for labels\n",
        "  num_subseqs = (len(corpus) - 1) // num_steps\n",
        "  # The starting indices for subsequences of length `num_steps`\n",
        "  intial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "\n",
        "  random.shuffle(intial_indices)\n",
        "\n",
        "\n",
        "  def data(pos):\n",
        "    return corpus[pos: pos + num_steps]\n",
        "\n",
        "\n",
        "  num_batches = num_subseqs // batch_size\n",
        "\n",
        "\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    # Here, `initial_indices` contains randomized starting indices for subsequences\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(j) for j in initial_indices_per_batch]\n",
        "    Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "    yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nxYJMutEjp-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_random():\n",
        "  pass\n",
        "\n",
        "\n",
        "def seq_data_iter_sequential():\n",
        "  pass\n",
        "\n",
        "\n",
        "def load_corpus_time_machine():\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  pass"
      ],
      "metadata": {
        "id": "rLosDK4tiYtK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}